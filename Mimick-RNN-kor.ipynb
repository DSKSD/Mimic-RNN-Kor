{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import Hangulpy as hg\n",
    "import datetime\n",
    "import pickle\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "from torch.nn.utils.rnn import PackedSequence,pack_padded_sequence\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretrained_vector = pickle.load(open(\"data/pretrained_word2vec.pkl\",\"rb\"))\n",
    "word2index = pickle.load(open(\"data/vocab_word2vec_300.dict\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index2word = {v:k for k,v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for i in range(len(index2word)):\n",
    "    vocab.append(index2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148392"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MimickRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab,word_embed,char_embed,char_hidden,mlp_hidden):\n",
    "        super(MimickRNN,self).__init__()\n",
    "        \n",
    "        self.word_embed = nn.Embedding(len(vocab),word_embed)\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        char_vocab = ['<pad>','<other>','ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', \n",
    "              'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', \n",
    "              'ㅎ', 'ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', \n",
    "              'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ',\n",
    "              'ㄳ', 'ㄵ', 'ㄶ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ',\n",
    "              'ㄾ', 'ㄿ', 'ㅀ', 'ㅄ',\n",
    "              '0','1','2','3','4','5','6','7','8','9','a','b','c','d','e','f','g','h','i','j','k',\n",
    "              'l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','A','B','C','D','E','F','G',\n",
    "              'H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z',\"{\",\"}\"\n",
    "               '-','(',')','!','~','?','[',']',',','.','/','<','>','#','@','$','%','^','&','*','_',\n",
    "               '+','-','=',':',';',\"'\",'\"']\n",
    "        \n",
    "        self.char_hidden = char_hidden\n",
    "        self.char2index = {v:i for i,v in enumerate(char_vocab)}\n",
    "        self.char_embed = nn.Embedding(len(self.char2index), char_embed)\n",
    "        self.mimick_rnn = nn.LSTM(char_embed,char_hidden,1,batch_first=True,bidirectional=True)\n",
    "        self.mimick_linear = nn.Sequential(nn.Linear(char_hidden*2,mlp_hidden),\n",
    "                                                           nn.Tanh(),\n",
    "                                                           nn.Linear(mlp_hidden,word_embed))\n",
    "        \n",
    "    def init_word_embed(self,pretrained_vectors):\n",
    "        self.word_embed.weight = nn.Parameter(torch.from_numpy(pretrained_vectors).float())\n",
    "        self.word_embed.requires_grad = False # 고정\n",
    "    \n",
    "    def init_char_hidden(self,size):\n",
    "        hidden = Variable(torch.zeros(2,size,self.char_hidden))\n",
    "        context = Variable(torch.zeros(2,size,self.char_hidden))\n",
    "        if USE_CUDA:\n",
    "            hidden = hidden.cuda()\n",
    "            context = hidden.cuda()\n",
    "        return hidden, context\n",
    "    \n",
    "    def prepare_single_char(self,token):\n",
    "        idxs=[]\n",
    "        for s in token:\n",
    "            if hg.is_hangul(s):\n",
    "                # 음소 단위 분해\n",
    "                try:\n",
    "                    emso = list(hg.decompose(s))\n",
    "                    if emso[-1]=='':\n",
    "                        emso.pop()\n",
    "                except:\n",
    "                    emso = s\n",
    "                idxs.extend(list(map(lambda w: self.char2index[w], emso)))\n",
    "            else:\n",
    "                candit=s\n",
    "                if s.isalpha():\n",
    "                    candit='<alpha>'\n",
    "                try:\n",
    "                    idxs.append(self.char2index[candit])\n",
    "                except:\n",
    "                    idxs.append(self.char2index['<other>']) # '' 가 OTHER같이\n",
    "        tensor = torch.LongTensor(idxs)\n",
    "        tensor = Variable(tensor)\n",
    "        return tensor\n",
    "    \n",
    "    def prepare_char(self,seq,index=None):\n",
    "        seq = list(map(lambda v: self.prepare_single_char(v), seq))\n",
    "        if index:\n",
    "            forsort = list(zip(seq,index))\n",
    "            forsort = sorted(forsort,key = lambda s: s[0].size(0),reverse=True)\n",
    "            seq,index = list(zip(*forsort))\n",
    "            seq,index = list(seq),list(index)\n",
    "        else:\n",
    "            seq = sorted(seq,key = lambda s: s.size(0),reverse=True)\n",
    "        length = [s.size(0) for s in seq]\n",
    "        max_length = max(length)\n",
    "        seq = [torch.cat([s,Variable(torch.LongTensor([self.char2index['<pad>']]*(max_length-s.size(0))))]).view(1,-1) for s in seq]\n",
    "        seq = torch.cat(seq)\n",
    "        if index:\n",
    "            return seq, length, Variable(torch.LongTensor(index))\n",
    "        else:\n",
    "            return seq, length\n",
    "        \n",
    "    def train_mimick(self,step,batch_size=32,lr=0.0001):\n",
    "        print(\"start training mimic-rnn with %d batch_size\" % batch_size)\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.parameters()),lr=lr)\n",
    "        for step_index in range(step):\n",
    "            try:\n",
    "                offset = 0\n",
    "                iter_index = list(range(len(self.vocab)//batch_size + 1))\n",
    "                for i in iter_index:\n",
    "                    voca = self.vocab[offset:offset+batch_size]\n",
    "                    index = list(range(offset,offset+batch_size))\n",
    "                    offset+=batch_size\n",
    "                    \n",
    "                    inputs, lengths, index = self.prepare_char(voca,index)\n",
    "                    if USE_CUDA:\n",
    "                        inputs = inputs.cuda()\n",
    "                        index = index.cuda()\n",
    "                    self.zero_grad()\n",
    "                    outputs = self.mimick(inputs,lengths)\n",
    "                    targets = self.word_embed(index)\n",
    "                    loss = F.mse_loss(outputs,targets)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    if i % 100==0:\n",
    "                        print(\"[%d/%d] [%d/%d] mean_loss : %.7f\" % (step_index,step,i,len(iter_index),loss.data[0]))\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Early Stop!\")\n",
    "                break\n",
    "            \n",
    "    def mimick(self,inputs,lengths):\n",
    "        hidden = self.init_char_hidden(inputs.size(0))\n",
    "        embedded = self.char_embed(inputs)\n",
    "        packed = pack_padded_sequence(embedded,lengths,batch_first=True)\n",
    "        outputs, (hidden,context) = self.mimick_rnn(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        hidden = torch.cat([h for h in hidden], 1) # concat\n",
    "        return self.mimick_linear(hidden)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MimickRNN(vocab,300,50,100,400) # vocab, word_embed, char_embed, char_hidden, mlp_hidden\n",
    "model.init_word_embed(pretrained_vector)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STEP = 50\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training mimic-rnn with 256 batch_size\n",
      "[0/50] [0/580] mean_loss : 0.0085805\n",
      "[0/50] [100/580] mean_loss : 0.0117477\n",
      "[0/50] [200/580] mean_loss : 0.0139335\n",
      "[0/50] [300/580] mean_loss : 0.0108910\n",
      "[0/50] [400/580] mean_loss : 0.0072766\n",
      "[0/50] [500/580] mean_loss : 0.0033978\n",
      "[1/50] [0/580] mean_loss : 0.0085238\n",
      "[1/50] [100/580] mean_loss : 0.0116398\n",
      "[1/50] [200/580] mean_loss : 0.0137777\n",
      "[1/50] [300/580] mean_loss : 0.0107359\n",
      "[1/50] [400/580] mean_loss : 0.0071467\n",
      "[1/50] [500/580] mean_loss : 0.0033152\n",
      "[2/50] [0/580] mean_loss : 0.0084109\n",
      "[2/50] [100/580] mean_loss : 0.0114618\n",
      "[2/50] [200/580] mean_loss : 0.0135673\n",
      "[2/50] [300/580] mean_loss : 0.0105518\n",
      "[2/50] [400/580] mean_loss : 0.0070053\n",
      "[2/50] [500/580] mean_loss : 0.0032310\n",
      "[3/50] [0/580] mean_loss : 0.0082807\n",
      "[3/50] [100/580] mean_loss : 0.0112699\n",
      "[3/50] [200/580] mean_loss : 0.0133459\n",
      "[3/50] [300/580] mean_loss : 0.0103625\n",
      "[3/50] [400/580] mean_loss : 0.0068629\n",
      "[3/50] [500/580] mean_loss : 0.0031478\n",
      "[4/50] [0/580] mean_loss : 0.0081455\n",
      "[4/50] [100/580] mean_loss : 0.0110743\n",
      "[4/50] [200/580] mean_loss : 0.0131223\n",
      "[4/50] [300/580] mean_loss : 0.0101732\n",
      "[4/50] [400/580] mean_loss : 0.0067216\n",
      "[4/50] [500/580] mean_loss : 0.0030662\n",
      "[5/50] [0/580] mean_loss : 0.0080094\n",
      "[5/50] [100/580] mean_loss : 0.0108788\n",
      "[5/50] [200/580] mean_loss : 0.0128995\n",
      "[5/50] [300/580] mean_loss : 0.0099857\n",
      "[5/50] [400/580] mean_loss : 0.0065824\n",
      "[5/50] [500/580] mean_loss : 0.0029865\n",
      "[6/50] [0/580] mean_loss : 0.0078739\n",
      "[6/50] [100/580] mean_loss : 0.0106850\n",
      "[6/50] [200/580] mean_loss : 0.0126789\n",
      "[6/50] [300/580] mean_loss : 0.0098006\n",
      "[6/50] [400/580] mean_loss : 0.0064455\n",
      "[6/50] [500/580] mean_loss : 0.0029086\n",
      "[7/50] [0/580] mean_loss : 0.0077397\n",
      "[7/50] [100/580] mean_loss : 0.0104935\n",
      "[7/50] [200/580] mean_loss : 0.0124612\n",
      "[7/50] [300/580] mean_loss : 0.0096182\n",
      "[7/50] [400/580] mean_loss : 0.0063111\n",
      "[7/50] [500/580] mean_loss : 0.0028325\n",
      "[8/50] [0/580] mean_loss : 0.0076072\n",
      "[8/50] [100/580] mean_loss : 0.0103046\n",
      "[8/50] [200/580] mean_loss : 0.0122465\n",
      "[8/50] [300/580] mean_loss : 0.0094387\n",
      "[8/50] [400/580] mean_loss : 0.0061792\n",
      "[8/50] [500/580] mean_loss : 0.0027582\n",
      "[9/50] [0/580] mean_loss : 0.0074765\n",
      "[9/50] [100/580] mean_loss : 0.0101187\n",
      "[9/50] [200/580] mean_loss : 0.0120350\n",
      "[9/50] [300/580] mean_loss : 0.0092621\n",
      "[9/50] [400/580] mean_loss : 0.0060498\n",
      "[9/50] [500/580] mean_loss : 0.0026856\n",
      "[10/50] [0/580] mean_loss : 0.0073478\n",
      "[10/50] [100/580] mean_loss : 0.0099356\n",
      "[10/50] [200/580] mean_loss : 0.0118266\n",
      "[10/50] [300/580] mean_loss : 0.0090883\n",
      "[10/50] [400/580] mean_loss : 0.0059228\n",
      "[10/50] [500/580] mean_loss : 0.0026148\n",
      "[11/50] [0/580] mean_loss : 0.0072211\n",
      "[11/50] [100/580] mean_loss : 0.0097555\n",
      "[11/50] [200/580] mean_loss : 0.0116214\n",
      "[11/50] [300/580] mean_loss : 0.0089173\n",
      "[11/50] [400/580] mean_loss : 0.0057982\n",
      "[11/50] [500/580] mean_loss : 0.0025457\n",
      "[12/50] [0/580] mean_loss : 0.0070965\n",
      "[12/50] [100/580] mean_loss : 0.0095784\n",
      "[12/50] [200/580] mean_loss : 0.0114192\n",
      "[12/50] [300/580] mean_loss : 0.0087490\n",
      "[12/50] [400/580] mean_loss : 0.0056760\n",
      "[12/50] [500/580] mean_loss : 0.0024782\n",
      "[13/50] [0/580] mean_loss : 0.0069738\n",
      "[13/50] [100/580] mean_loss : 0.0094043\n",
      "[13/50] [200/580] mean_loss : 0.0112200\n",
      "[13/50] [300/580] mean_loss : 0.0085835\n",
      "[13/50] [400/580] mean_loss : 0.0055561\n",
      "[13/50] [500/580] mean_loss : 0.0024124\n",
      "[14/50] [0/580] mean_loss : 0.0068530\n",
      "[14/50] [100/580] mean_loss : 0.0092332\n",
      "[14/50] [200/580] mean_loss : 0.0110238\n",
      "[14/50] [300/580] mean_loss : 0.0084207\n",
      "[14/50] [400/580] mean_loss : 0.0054384\n",
      "[14/50] [500/580] mean_loss : 0.0023482\n",
      "[15/50] [0/580] mean_loss : 0.0067340\n",
      "[15/50] [100/580] mean_loss : 0.0090650\n",
      "[15/50] [200/580] mean_loss : 0.0108304\n",
      "[15/50] [300/580] mean_loss : 0.0082606\n",
      "[15/50] [400/580] mean_loss : 0.0053229\n",
      "[15/50] [500/580] mean_loss : 0.0022856\n",
      "[16/50] [0/580] mean_loss : 0.0066169\n",
      "[16/50] [100/580] mean_loss : 0.0088998\n",
      "[16/50] [200/580] mean_loss : 0.0106400\n",
      "[16/50] [300/580] mean_loss : 0.0081032\n",
      "[16/50] [400/580] mean_loss : 0.0052095\n",
      "[16/50] [500/580] mean_loss : 0.0022247\n",
      "[17/50] [0/580] mean_loss : 0.0065016\n",
      "[17/50] [100/580] mean_loss : 0.0087374\n",
      "[17/50] [200/580] mean_loss : 0.0104524\n",
      "[17/50] [300/580] mean_loss : 0.0079484\n",
      "[17/50] [400/580] mean_loss : 0.0050982\n",
      "[17/50] [500/580] mean_loss : 0.0021654\n",
      "[18/50] [0/580] mean_loss : 0.0063882\n",
      "[18/50] [100/580] mean_loss : 0.0085778\n",
      "[18/50] [200/580] mean_loss : 0.0102676\n",
      "[18/50] [300/580] mean_loss : 0.0077962\n",
      "[18/50] [400/580] mean_loss : 0.0049889\n",
      "[18/50] [500/580] mean_loss : 0.0021078\n",
      "[19/50] [0/580] mean_loss : 0.0062766\n",
      "[19/50] [100/580] mean_loss : 0.0084212\n",
      "[19/50] [200/580] mean_loss : 0.0100858\n",
      "[19/50] [300/580] mean_loss : 0.0076467\n",
      "[19/50] [400/580] mean_loss : 0.0048816\n",
      "[19/50] [500/580] mean_loss : 0.0020519\n",
      "[20/50] [0/580] mean_loss : 0.0061672\n",
      "[20/50] [100/580] mean_loss : 0.0082675\n",
      "[20/50] [200/580] mean_loss : 0.0099069\n",
      "[20/50] [300/580] mean_loss : 0.0074996\n",
      "[20/50] [400/580] mean_loss : 0.0047763\n",
      "[20/50] [500/580] mean_loss : 0.0019977\n",
      "[21/50] [0/580] mean_loss : 0.0060603\n",
      "[21/50] [100/580] mean_loss : 0.0081168\n",
      "[21/50] [200/580] mean_loss : 0.0097309\n",
      "[21/50] [300/580] mean_loss : 0.0073551\n",
      "[21/50] [400/580] mean_loss : 0.0046728\n",
      "[21/50] [500/580] mean_loss : 0.0019450\n",
      "[22/50] [0/580] mean_loss : 0.0059561\n",
      "[22/50] [100/580] mean_loss : 0.0079694\n",
      "[22/50] [200/580] mean_loss : 0.0095577\n",
      "[22/50] [300/580] mean_loss : 0.0072131\n",
      "[22/50] [400/580] mean_loss : 0.0045713\n",
      "[22/50] [500/580] mean_loss : 0.0018931\n",
      "[23/50] [0/580] mean_loss : 0.0058542\n",
      "[23/50] [100/580] mean_loss : 0.0078256\n",
      "[23/50] [200/580] mean_loss : 0.0093874\n",
      "[23/50] [300/580] mean_loss : 0.0070735\n",
      "[23/50] [400/580] mean_loss : 0.0044717\n",
      "[23/50] [500/580] mean_loss : 0.0018408\n",
      "[24/50] [0/580] mean_loss : 0.0057537\n",
      "[24/50] [100/580] mean_loss : 0.0076860\n",
      "[24/50] [200/580] mean_loss : 0.0092200\n",
      "[24/50] [300/580] mean_loss : 0.0069365\n",
      "[24/50] [400/580] mean_loss : 0.0043741\n",
      "[24/50] [500/580] mean_loss : 0.0017902\n",
      "[25/50] [0/580] mean_loss : 0.0056706\n",
      "[25/50] [100/580] mean_loss : 0.0075510\n",
      "[25/50] [200/580] mean_loss : 0.0090555\n",
      "[25/50] [300/580] mean_loss : 0.0068020\n",
      "[25/50] [400/580] mean_loss : 0.0042783\n",
      "[25/50] [500/580] mean_loss : 0.0017540\n",
      "[26/50] [0/580] mean_loss : 0.0056025\n",
      "[26/50] [100/580] mean_loss : 0.0074208\n",
      "[26/50] [200/580] mean_loss : 0.0088933\n",
      "[26/50] [300/580] mean_loss : 0.0066697\n",
      "[26/50] [400/580] mean_loss : 0.0041840\n",
      "[26/50] [500/580] mean_loss : 0.0017150\n",
      "[27/50] [0/580] mean_loss : 0.0055013\n",
      "[27/50] [100/580] mean_loss : 0.0072904\n",
      "[27/50] [200/580] mean_loss : 0.0087313\n",
      "[27/50] [300/580] mean_loss : 0.0065394\n",
      "[27/50] [400/580] mean_loss : 0.0040919\n",
      "[27/50] [500/580] mean_loss : 0.0016627\n",
      "[28/50] [0/580] mean_loss : 0.0053938\n",
      "[28/50] [100/580] mean_loss : 0.0071561\n",
      "[28/50] [200/580] mean_loss : 0.0085720\n",
      "[28/50] [300/580] mean_loss : 0.0064117\n",
      "[28/50] [400/580] mean_loss : 0.0040018\n",
      "[28/50] [500/580] mean_loss : 0.0016169\n",
      "[29/50] [0/580] mean_loss : 0.0052948\n",
      "[29/50] [100/580] mean_loss : 0.0070207\n",
      "[29/50] [200/580] mean_loss : 0.0084174\n",
      "[29/50] [300/580] mean_loss : 0.0062863\n",
      "[29/50] [400/580] mean_loss : 0.0039135\n",
      "[29/50] [500/580] mean_loss : 0.0015746\n",
      "[30/50] [0/580] mean_loss : 0.0052034\n",
      "[30/50] [100/580] mean_loss : 0.0068885\n",
      "[30/50] [200/580] mean_loss : 0.0082661\n",
      "[30/50] [300/580] mean_loss : 0.0061628\n",
      "[30/50] [400/580] mean_loss : 0.0038270\n",
      "[30/50] [500/580] mean_loss : 0.0015341\n",
      "[31/50] [0/580] mean_loss : 0.0051182\n",
      "[31/50] [100/580] mean_loss : 0.0067616\n",
      "[31/50] [200/580] mean_loss : 0.0081174\n",
      "[31/50] [300/580] mean_loss : 0.0060420\n",
      "[31/50] [400/580] mean_loss : 0.0037424\n",
      "[31/50] [500/580] mean_loss : 0.0014981\n",
      "[32/50] [0/580] mean_loss : 0.0050400\n",
      "[32/50] [100/580] mean_loss : 0.0066397\n",
      "[32/50] [200/580] mean_loss : 0.0079739\n",
      "[32/50] [300/580] mean_loss : 0.0059234\n",
      "[32/50] [400/580] mean_loss : 0.0036598\n",
      "[32/50] [500/580] mean_loss : 0.0014626\n",
      "[33/50] [0/580] mean_loss : 0.0049692\n",
      "[33/50] [100/580] mean_loss : 0.0065221\n",
      "[33/50] [200/580] mean_loss : 0.0078352\n",
      "[33/50] [300/580] mean_loss : 0.0058062\n",
      "[33/50] [400/580] mean_loss : 0.0035825\n",
      "[33/50] [500/580] mean_loss : 0.0014186\n",
      "[34/50] [0/580] mean_loss : 0.0048997\n",
      "[34/50] [100/580] mean_loss : 0.0064070\n",
      "[34/50] [200/580] mean_loss : 0.0076942\n",
      "[34/50] [300/580] mean_loss : 0.0056919\n",
      "[34/50] [400/580] mean_loss : 0.0035151\n",
      "[34/50] [500/580] mean_loss : 0.0013728\n",
      "[35/50] [0/580] mean_loss : 0.0048209\n",
      "[35/50] [100/580] mean_loss : 0.0062934\n",
      "[35/50] [200/580] mean_loss : 0.0075491\n",
      "[35/50] [300/580] mean_loss : 0.0055839\n",
      "[35/50] [400/580] mean_loss : 0.0034486\n",
      "[35/50] [500/580] mean_loss : 0.0013608\n",
      "[36/50] [0/580] mean_loss : 0.0047364\n",
      "[36/50] [100/580] mean_loss : 0.0061827\n",
      "[36/50] [200/580] mean_loss : 0.0074062\n",
      "[36/50] [300/580] mean_loss : 0.0054856\n",
      "[36/50] [400/580] mean_loss : 0.0033633\n",
      "[36/50] [500/580] mean_loss : 0.0013744\n",
      "[37/50] [0/580] mean_loss : 0.0046478\n",
      "[37/50] [100/580] mean_loss : 0.0060649\n",
      "[37/50] [200/580] mean_loss : 0.0072710\n",
      "[37/50] [300/580] mean_loss : 0.0053898\n",
      "[37/50] [400/580] mean_loss : 0.0032765\n",
      "[37/50] [500/580] mean_loss : 0.0013410\n",
      "[38/50] [0/580] mean_loss : 0.0045594\n",
      "[38/50] [100/580] mean_loss : 0.0059423\n",
      "[38/50] [200/580] mean_loss : 0.0071380\n",
      "[38/50] [300/580] mean_loss : 0.0052886\n",
      "[38/50] [400/580] mean_loss : 0.0032016\n",
      "[38/50] [500/580] mean_loss : 0.0012865\n",
      "[39/50] [0/580] mean_loss : 0.0044787\n",
      "[39/50] [100/580] mean_loss : 0.0058248\n",
      "[39/50] [200/580] mean_loss : 0.0070046\n",
      "[39/50] [300/580] mean_loss : 0.0051804\n",
      "[39/50] [400/580] mean_loss : 0.0031285\n",
      "[39/50] [500/580] mean_loss : 0.0012304\n",
      "[40/50] [0/580] mean_loss : 0.0044036\n",
      "[40/50] [100/580] mean_loss : 0.0057125\n",
      "[40/50] [200/580] mean_loss : 0.0068713\n",
      "[40/50] [300/580] mean_loss : 0.0050703\n",
      "[40/50] [400/580] mean_loss : 0.0030586\n",
      "[40/50] [500/580] mean_loss : 0.0011800\n",
      "[41/50] [0/580] mean_loss : 0.0043298\n",
      "[41/50] [100/580] mean_loss : 0.0056009\n",
      "[41/50] [200/580] mean_loss : 0.0067406\n",
      "[41/50] [300/580] mean_loss : 0.0049626\n",
      "[41/50] [400/580] mean_loss : 0.0029946\n",
      "[41/50] [500/580] mean_loss : 0.0011404\n",
      "[42/50] [0/580] mean_loss : 0.0042559\n",
      "[42/50] [100/580] mean_loss : 0.0054889\n",
      "[42/50] [200/580] mean_loss : 0.0066139\n",
      "[42/50] [300/580] mean_loss : 0.0048585\n",
      "[42/50] [400/580] mean_loss : 0.0029341\n",
      "[42/50] [500/580] mean_loss : 0.0011110\n",
      "[43/50] [0/580] mean_loss : 0.0041825\n",
      "[43/50] [100/580] mean_loss : 0.0053786\n",
      "[43/50] [200/580] mean_loss : 0.0064910\n",
      "[43/50] [300/580] mean_loss : 0.0047576\n",
      "[43/50] [400/580] mean_loss : 0.0028729\n",
      "[43/50] [500/580] mean_loss : 0.0010873\n",
      "[44/50] [0/580] mean_loss : 0.0041109\n",
      "[44/50] [100/580] mean_loss : 0.0052719\n",
      "[44/50] [200/580] mean_loss : 0.0063712\n",
      "[44/50] [300/580] mean_loss : 0.0046591\n",
      "[44/50] [400/580] mean_loss : 0.0028091\n",
      "[44/50] [500/580] mean_loss : 0.0010645\n",
      "[45/50] [0/580] mean_loss : 0.0040437\n",
      "[45/50] [100/580] mean_loss : 0.0051695\n",
      "[45/50] [200/580] mean_loss : 0.0062537\n",
      "[45/50] [300/580] mean_loss : 0.0045621\n",
      "[45/50] [400/580] mean_loss : 0.0027440\n",
      "[45/50] [500/580] mean_loss : 0.0010402\n",
      "[46/50] [0/580] mean_loss : 0.0039829\n",
      "[46/50] [100/580] mean_loss : 0.0050710\n",
      "[46/50] [200/580] mean_loss : 0.0061379\n",
      "[46/50] [300/580] mean_loss : 0.0044668\n",
      "[46/50] [400/580] mean_loss : 0.0026813\n",
      "[46/50] [500/580] mean_loss : 0.0010149\n",
      "[47/50] [0/580] mean_loss : 0.0039266\n",
      "[47/50] [100/580] mean_loss : 0.0049740\n",
      "[47/50] [200/580] mean_loss : 0.0060236\n",
      "[47/50] [300/580] mean_loss : 0.0043744\n",
      "[47/50] [400/580] mean_loss : 0.0026239\n",
      "[47/50] [500/580] mean_loss : 0.0009917\n",
      "[48/50] [0/580] mean_loss : 0.0038716\n",
      "[48/50] [100/580] mean_loss : 0.0048779\n",
      "[48/50] [200/580] mean_loss : 0.0059100\n",
      "[48/50] [300/580] mean_loss : 0.0042864\n",
      "[48/50] [400/580] mean_loss : 0.0025729\n",
      "[48/50] [500/580] mean_loss : 0.0009714\n",
      "[49/50] [0/580] mean_loss : 0.0038156\n",
      "[49/50] [100/580] mean_loss : 0.0047836\n",
      "[49/50] [200/580] mean_loss : 0.0057981\n",
      "[49/50] [300/580] mean_loss : 0.0042032\n",
      "[49/50] [400/580] mean_loss : 0.0025260\n",
      "[49/50] [500/580] mean_loss : 0.0009508\n"
     ]
    }
   ],
   "source": [
    "model.train_mimick(STEP,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test Rare word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ko.wikipedia.org/wiki/%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD%EC%9D%98_%EC%9D%B8%ED%84%B0%EB%84%B7_%EC%8B%A0%EC%A1%B0%EC%96%B4_%EB%AA%A9%EB%A1%9D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rare_words = [\"가즈아\",\"고고씽\",\"갠톡\",\"급식충\",\"꿀잼\",\"존잼\",\"존잘\",\"낫닝겐\",\"덕통사고\",\"먹튀\",\"보이루\",\"빝코\",\"띵작\",\"냥이\",\"키드밀리\",\n",
    "                    \"빈스빈스\",\"ㅂㅇㄹ\",\"아이오아이\",\"동동키드\",\"안뇽\",\"옼돜\",\"띵언\",\"블랙핑크\",\"프사기\",\"선우정아\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가즈아 is not in voca\n",
      "갠톡 is not in voca\n",
      "급식충 is not in voca\n",
      "꿀잼 is not in voca\n",
      "존잼 is not in voca\n",
      "존잘 is not in voca\n",
      "낫닝겐 is not in voca\n",
      "덕통사고 is not in voca\n",
      "먹튀 is not in voca\n",
      "보이루 is not in voca\n",
      "빝코 is not in voca\n",
      "띵작 is not in voca\n",
      "냥이 is not in voca\n",
      "키드밀리 is not in voca\n",
      "빈스빈스 is not in voca\n",
      "ㅂㅇㄹ is not in voca\n",
      "아이오아이 is not in voca\n",
      "동동키드 is not in voca\n",
      "안뇽 is not in voca\n",
      "옼돜 is not in voca\n",
      "띵언 is not in voca\n",
      "블랙핑크 is not in voca\n",
      "프사기 is not in voca\n",
      "선우정아 is not in voca\n"
     ]
    }
   ],
   "source": [
    "for word in rare_words:\n",
    "    try:\n",
    "        model.vocab.index(word)\n",
    "    except:\n",
    "        print(\"%s is not in voca\" % word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_most_word_embedding(word,num=5):\n",
    "    matrix = model.word_embed.weight\n",
    "    inputs,lengths = model.prepare_char([word])\n",
    "    if USE_CUDA: inputs = inputs.cuda()\n",
    "    embedding = model.mimick(inputs,lengths)\n",
    "    similarities = matrix.matmul(embedding.transpose(0,1))\n",
    "    similarities = similarities.transpose(0,1)\n",
    "    norm = matrix.norm(dim=1)*embedding.norm()\n",
    "    similarities = similarities/norm\n",
    "    \n",
    "    _ , i = similarities.topk(num)\n",
    "    index = i.data.tolist()[0]\n",
    "    similar_words = [model.vocab[i] for i in index]\n",
    "    print(word)\n",
    "    print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가즈아\n",
      "['가와나', '야바시라', '후타라', '고즈카', '기타우라']\n",
      "\n",
      "\n",
      "고고씽\n",
      "['밥투정', '압슬형', '훈시규정', '가오핑', '언필칭']\n",
      "\n",
      "\n",
      "갠톡\n",
      "['해칭', '거폭', '버섯바위', '뎃전', '저팬타운']\n",
      "\n",
      "\n",
      "급식충\n",
      "['간흡충', '십이지장충', '톡소포자충', '활축', '부식기']\n",
      "\n",
      "\n",
      "꿀잼\n",
      "['꼬리별', '가래미', '감천만', '큰입구몸', '비늘돔']\n",
      "\n",
      "\n",
      "존잼\n",
      "['전소호', '손인형', '이무성', '임두성', '조한구']\n",
      "\n",
      "\n",
      "존잘\n",
      "['손절', '약반', '곁말', '세변', '중탈']\n",
      "\n",
      "\n",
      "낫닝겐\n",
      "['메허샬레하쉬바즈', '뱌쿠엔', '오도노반', '렝가만', '하이위안']\n",
      "\n",
      "\n",
      "덕통사고\n",
      "['중간보고', '남사고', '동문휘고', '부사원', '동정남']\n",
      "\n",
      "\n",
      "먹튀\n",
      "['밭머리', '모래그릇', '새앙머리', '도들', '킬킬거리']\n",
      "\n",
      "\n",
      "보이루\n",
      "['캰도루', '다보아', '모리우', '소우리', '탐모라']\n",
      "\n",
      "\n",
      "빝코\n",
      "['펨코', '필리펜코', '올메카', '헵토미노', '줄라이카']\n",
      "\n",
      "\n",
      "띵작\n",
      "['띵호', '와작와작', '매타작', '시작', '팔건장']\n",
      "\n",
      "\n",
      "냥이\n",
      "['샹이', '뇽이', '멩이', '팔랑이', '쨩이']\n",
      "\n",
      "\n",
      "키드밀리\n",
      "['게이틀리', '케어필리', '킹즐리', '트링코말리', '몰로카이']\n",
      "\n",
      "\n",
      "빈스빈스\n",
      "['블레빈스', '라데팡스', '스타인펠드', '게이틀리', '부캐넌']\n",
      "\n",
      "\n",
      "ㅂㅇㄹ\n",
      "['흑', '죽', '걱', '잡', 'ㅇㅅㅁ']\n",
      "\n",
      "\n",
      "아이오아이\n",
      "['만타가오리', '아이조메', '아노아이', '아메노모리', '안가라노']\n",
      "\n",
      "\n",
      "동동키드\n",
      "['월포드', '도거뱅크', '민톤', '아칸서스', '위태로운']\n",
      "\n",
      "\n",
      "안뇽\n",
      "['안롱', '우망', '후란', '나궁', '세롱']\n",
      "\n",
      "\n",
      "옼돜\n",
      "['낰낰', '폿푸루', '가로놓이', '바께쓰', '시갓']\n",
      "\n",
      "\n",
      "띵언\n",
      "['띵호', '떠본', '나무저', '필은', '펍에']\n",
      "\n",
      "\n",
      "블랙핑크\n",
      "['메이플스', '스펀지케이크', '플링크', '크랜필드', '스푹스']\n",
      "\n",
      "\n",
      "프사기\n",
      "['엑타', '프린트기', '시데리아기', '우레기', '요타바이트']\n",
      "\n",
      "\n",
      "선우정아\n",
      "['공민영', '주임영', '전지연', '강무영', '박세인']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in rare_words:\n",
    "    get_most_word_embedding(word)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdate = datetime.datetime.strftime(datetime.datetime.now(),\"%m_%d\")\n",
    "config = {'model_path' : '/models/mimick_params_'+cdate+'.model','vocab_path' : '/models/mimick_'+cdate+'.vocab', 'word_embed' : 300, 'char_embed' : 50, 'char_hidden' : 100, 'mlp_hidden' : 400}\n",
    "\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cpu()\n",
    "    \n",
    "torch.save(model.state_dict(), 'models/mimick_params_'+cdate+'.model')\n",
    "pickle.dump(config,open('models/mimick_'+cdate+'.config',\"wb\"))\n",
    "pickle.dump(vocab,open('models/mimick_'+cdate+'.vocab',\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
